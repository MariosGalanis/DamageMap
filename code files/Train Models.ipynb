{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Train Models.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ef6aohhOgxI5"},"source":["*   The present script was developed and used on Google Colab. The purpose of the script is to allow the user to train the models described in our paper \"DamageMap: A post-wildfire damaged buildings classifier\". The models will output \"0\" for an undamaged building, and \"1\" for a damaged building.\n","\n","When training the models to classify the images of a chosen dataset, the dataset should consist of separate images of building roofs. The user should prepare the dataset in the following way:\n","\n","*   The images of the dataset should be placed in a folder that contains 2 subfolders. The first (in alphabetical order) subfolder should contain the images of the undamaged buildings, because they will automatically get the label \"0\" (and we want the model to predict \"0\" for undamaged buildings). The second (in alphabetical order) subfolder should contain the images of damaged buildings."]},{"cell_type":"markdown","metadata":{"id":"4IGeQB6Eh0lZ"},"source":["The following cell allows Google Colab to get access to the files of your Google Drive."]},{"cell_type":"code","metadata":{"id":"5-64WDWO7Rb8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620597590037,"user_tz":420,"elapsed":809,"user":{"displayName":"Mar Gal","photoUrl":"","userId":"14196548149658065855"}},"outputId":"88e09f9d-7ee9-4cc8-b97d-4ea2c9984e49"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","%cd drive/My\\ Drive"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O3Lmmo6EiRxd"},"source":["Importing *necessary* libraries."]},{"cell_type":"code","metadata":{"id":"T9knxogh7fQJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620597592158,"user_tz":420,"elapsed":1179,"user":{"displayName":"Mar Gal","photoUrl":"","userId":"14196548149658065855"}},"outputId":"eb123a29-3a8e-4dcc-c32e-02db0be452ab"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data import sampler, RandomSampler, SubsetRandomSampler\n","from torch.utils.tensorboard import SummaryWriter\n","from PIL import Image, ImageOps\n","import torchvision.datasets as dset\n","import torchvision.transforms as T\n","import cv2\n","\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","import time\n","\n","import seaborn as sns\n","from __future__ import print_function \n","from __future__ import division\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["PyTorch Version:  1.8.1+cu101\n","Torchvision Version:  0.9.1+cu101\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Sv5gik5CiYaG"},"source":["If a GPU is available then the following cell will allow our model to use it, to train faster. It is not advised to train such a deep network if a GPU is not available."]},{"cell_type":"code","metadata":{"id":"-hksaBf3FRRf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620597596522,"user_tz":420,"elapsed":267,"user":{"displayName":"Mar Gal","photoUrl":"","userId":"14196548149658065855"}},"outputId":"acea6de6-819b-48a0-92ae-43228e89ae63"},"source":["USE_GPU = True\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","print('using device:', device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using device: cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mocXDL34_PmL"},"source":["Load and prepare the dataset that the model will train on."]},{"cell_type":"code","metadata":{"id":"OmaD6YHtfUAG"},"source":["FOLDERNAME = 'damaged_structures_detector/xbd_images'      # This folder contains 3 subfolders with the 'train', 'val', and 'test' sets. Each one consists of two subfolders, one for undamaged (first alphabetically) and one for damaged bulding images.\n","BATCH_SIZE = 128                                           # Capped by GPU memory\n","\n","first_transform = transforms.Compose([                     # This is the first transform that we apply to the dataset. It just resizes and crops the building images\n","        transforms.Resize(224),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor()\n","    ])\n","\n","train_set = datasets.ImageFolder(root=os.path.join(FOLDERNAME,'train'), transform = first_transform)\n","train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, num_workers = 16)\n","\n","# The next lines calculate the mean and standard deviation of the training set images. We will later normalize the images based on these two values.\n","mean = 0.\n","std = 0.\n","nb_samples = 0.\n","for data, labels in train_loader:\n","    batch_samples = data.size(0)\n","    data = data.view(batch_samples, data.size(1), -1)\n","    mean += data.mean(2).sum(0)\n","    std += data.std(2).sum(0)\n","    nb_samples += batch_samples\n","\n","mean /= nb_samples\n","std /= nb_samples"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A-8JPuMFolZU"},"source":["The next cell applies the second transformation (the one described in the paper) and prepares the dataset for the training process."]},{"cell_type":"code","metadata":{"id":"QcBFcDMugX38"},"source":["data_transform = transforms.Compose([       # This is the 2nd and last transformation. It contains the addition of random noise and the normalization by the mean and std of the train set\n","        transforms.Resize(224),\n","        transforms.CenterCrop(224),\n","        transforms.ColorJitter(brightness=0.7, contrast=0.6, saturation=0.3, hue=0),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=mean,\n","                             std=std)\n","    ])\n","\n","train_dataset = datasets.ImageFolder(root=os.path.join(FOLDERNAME,'train'),transform = data_transform)  # using this transformation we prepare the dataset on which we will train the model\n","val_dataset = datasets.ImageFolder(root=os.path.join(FOLDERNAME,'val'),transform = data_transform)      # using this transformation we prepare the dataset on which we will tune the training hyperparameters\n","test_dataset = datasets.ImageFolder(root=os.path.join(FOLDERNAME,'test'),transform = data_transform)    # using this transformation we prepare the dataset on which we will evaluate the model's performance"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9dmIFPBZpJYq"},"source":["The following cell is just repeating the previous 3 lines, because sometimes Google Colab fails to load datasets in first attempt"]},{"cell_type":"code","metadata":{"id":"ETYXDJwNunuO"},"source":["train_dataset = datasets.ImageFolder(root=os.path.join(FOLDERNAME,'train'),transform = data_transform)\n","val_dataset = datasets.ImageFolder(root=os.path.join(FOLDERNAME,'val'),transform = data_transform)\n","test_dataset = datasets.ImageFolder(root=os.path.join(FOLDERNAME,'test'),transform = data_transform)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yp766PHK_p_A"},"source":["In the next few cells we are using the created datasets to prepare the Pytorch Dataloaders"]},{"cell_type":"code","metadata":{"id":"rnO0fihdFKGQ"},"source":["# Intended Input Image Size\n","HEIGHT = 224\n","WIDTH = 224\n","CHANNELS = 3\n","\n","# Number of training examples\n","NUM_TRAIN = len(train_dataset)\n","\n","# Number of Validation examples\n","NUM_VAL = len(val_dataset)\n","\n","# Number of Test Examples\n","NUM_TEST = len(test_dataset)\n","\n","# Batch Size for Training (capped by GPU)\n","BATCH_SIZE = 64\n","\n","\n","# Shuffle the Data with Random Samplers\n","train_indices = list(range(NUM_TRAIN))\n","np.random.shuffle(train_indices)\n","\n","val_indices = list(range(NUM_VAL))\n","np.random.shuffle(val_indices)\n","\n","test_indices = list(range(NUM_TEST))\n","np.random.shuffle(test_indices)\n","\n","train_idx, val_idx, test_idx = train_indices, val_indices, test_indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZy7wZgv_DTU"},"source":["train_sampler = SubsetRandomSampler(train_idx)\n","val_sampler   = SubsetRandomSampler(val_idx)\n","test_sampler  = SubsetRandomSampler(test_idx)\n","\n","\n","loader_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers = 4) ## can switch to \"shuffle = True\" if sampler does not work \n","\n","loader_val = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers = 4)\n","\n","loader_test = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, num_workers = 4)\n","\n","\n","# Creating a dictionary that contains the train-val-test dataloaders, to call them efficiently when neccessary\n","dataloaders_dict = {}\n","dataloaders_dict.update( {'train' : loader_train} )\n","dataloaders_dict.update( {'val' : loader_val} )\n","dataloaders_dict.update( {'test' : loader_test} )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"22OI1dbZ2iVq","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594672161008,"user_tz":420,"elapsed":158812,"user":{"displayName":"Mar Gal","photoUrl":"","userId":"14196548149658065855"}},"outputId":"e8d041ab-5e21-4340-b7af-59f68e37751a"},"source":["loader_val.dataset.classes  # just printing the two classes to notice that we put undamaged folder 1st to take the \"0\" output!"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['a_not_destroyed', 'b_destroyed']"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"_x3W0vwzKQTA"},"source":["Decide the type of the Model that we will train and a few other parameters"]},{"cell_type":"code","metadata":{"id":"8_2FI9QG_LIK"},"source":["# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n","MODEL_NAME = \"resnet\"\n","\n","# Number of classes in the dataset\n","NUM_CLASSES = 2 # (undamaged \"0\", damaged \"1\")\n","\n","# Flag for feature extracting. When False, we finetune the whole model, \n","# when True we only update the reshaped (output) layer params\n","FEATURE_EXTRACT = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SikQpStcq9a7"},"source":["The following helper function sets the ``.requires_grad`` attribute of the\n","parameters in the model to False when we are feature extracting. By\n","default, when we load a pretrained model all of the parameters have\n","``.requires_grad=True``, which is fine if we are training from scratch\n","or finetuning. However, if we are feature extracting and only want to\n","compute gradients for the newly initialized layer then we want all of\n","the other parameters to not require gradients."]},{"cell_type":"code","metadata":{"id":"exHCDxHnBg1U"},"source":["def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B3CNLWCqCJ_Y"},"source":["The Following Function takes the model name, input dimensions, number of classes, feature extraction or finetuning, and pretrained or not and initializes the model. It only requires that the input images will have a square shape and even number of pixels in each dimensions (can be easily modified if input needs to be different)."]},{"cell_type":"code","metadata":{"id":"Z1MON0qQBpj1"},"source":["def initialize_model(model_name, num_classes, width, channels, feature_extract, use_pretrained=True):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == \"resnet\":\n","        \"\"\" Resnet18\n","        \"\"\"                \n","        model_ft = models.resnet18(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","        if(width != input_size):       \n","          W1 = width\n","          if (input_size > W1):\n","            F = 1\n","            P = int((input_size - W1) / 2)\n","          else:\n","            P = 0\n","            F = W1 - input_size +1\n","          \n","          first_conv_layer = nn.Conv2d(channels, 3, kernel_size=F, stride=1, padding=P, dilation=1, groups=1, bias=True)\n","          model_ft = nn.Sequential(first_conv_layer, model_ft)\n","  \n","\n","    elif model_name == \"alexnet\":\n","        \"\"\" Alexnet\n","        \"\"\"\n","        model_ft = models.alexnet(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","        if(width != input_size):       \n","          W1 = width\n","          if (input_size > W1):\n","            F = 1\n","            P = int((input_size - W1) / 2)\n","          else:\n","            P = 0\n","            F = W1 - input_size +1\n","          \n","          first_conv_layer = [nn.Conv2d(channels, 3, kernel_size=F, stride=1, padding=P, dilation=1, groups=1, bias=True)]\n","          first_conv_layer.extend(list(model_ft.features))  \n","          model_ft.features= nn.Sequential(*first_conv_layer)\n","\n","    elif model_name == \"vgg\":\n","        \"\"\" VGG11_bn\n","        \"\"\"\n","        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","        if(width != input_size):       \n","          W1 = width\n","          if (input_size > W1):\n","            F = 1\n","            P = int((input_size - W1) / 2)\n","          else:\n","            P = 0\n","            F = W1 - input_size +1\n","          \n","          first_conv_layer = [nn.Conv2d(channels, 3, kernel_size=F, stride=1, padding=P, dilation=1, groups=1, bias=True)]\n","          first_conv_layer.extend(list(model_ft.features))  \n","          model_ft.features= nn.Sequential(*first_conv_layer)\n","\n","    elif model_name == \"squeezenet\":\n","        \"\"\" Squeezenet\n","        \"\"\"\n","        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n","        model_ft.num_classes = num_classes\n","        input_size = 224\n","\n","        if(width != input_size):       \n","          W1 = width\n","          if (input_size > W1):\n","            F = 1\n","            P = int((input_size - W1) / 2)\n","          else:\n","            P = 0\n","            F = W1 - input_size +1\n","          \n","          first_conv_layer = [nn.Conv2d(channels, 3, kernel_size=F, stride=1, padding=P, dilation=1, groups=1, bias=True)]\n","          first_conv_layer.extend(list(model_ft.features))  \n","          model_ft.features= nn.Sequential(*first_conv_layer)\n","        \n","\n","    elif model_name == \"densenet\":\n","        \"\"\" Densenet\n","        \"\"\"\n","        model_ft = models.densenet121(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier.in_features\n","        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n","        input_size = 224\n","\n","        if(width != input_size):       \n","          W1 = width\n","          if (input_size > W1):\n","            F = 1\n","            P = int((input_size - W1) / 2)\n","          else:\n","            P = 0\n","            F = W1 - input_size +1\n","          \n","          first_conv_layer = [nn.Conv2d(channels, 3, kernel_size=F, stride=1, padding=P, dilation=1, groups=1, bias=True)]\n","          first_conv_layer.extend(list(model_ft.features))  \n","          model_ft.features= nn.Sequential(*first_conv_layer)\n","\n","    elif model_name == \"inception\":\n","        \"\"\" Inception v3 \n","        Be careful, expects (299,299) sized images and has auxiliary output\n","        \"\"\"\n","        model_ft = models.inception_v3(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        # Handle the auxilary net\n","        num_ftrs = model_ft.AuxLogits.fc.in_features\n","        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","        # Handle the primary net\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n","        input_size = 299\n","\n","        if(width != input_size):       \n","          W1 = width\n","          if (input_size > W1):\n","            F = 1\n","            P = int((input_size - W1) / 2)\n","          else:\n","            P = 0\n","            F = W1 - input_size +1\n","          \n","          first_conv_layer = nn.Conv2d(channels, 3, kernel_size=F, stride=1, padding=P, dilation=1, groups=1, bias=True)\n","          model_ft.AuxLogits = nn.Sequential(first_conv_layer, model_ft.AuxLogits)\n","          model_ft = nn.Sequential(first_conv_layer, model_ft)\n","\n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","    \n","    return model_ft, input_size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gIHA_P3jUGN5"},"source":["The next cell initializes a new model."]},{"cell_type":"code","metadata":{"id":"VM2UFalLK7Dv"},"source":["%%capture\n","# Initialize the model for training\n","\n","model_ft, input_size = initialize_model(MODEL_NAME, NUM_CLASSES, WIDTH, CHANNELS, FEATURE_EXTRACT, use_pretrained=True)\n","\n","# uncomment the next 2 lines if you want to print the parameters of the model we just instantiated\n","#for param in model_ft.parameters():\n","#  print(param.data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H3b1j1eyOXBX"},"source":["The next cell loads the latest state of the model we are about to train. Run it only if you have saved some checkpoints of the same model from previous training sessions."]},{"cell_type":"code","metadata":{"id":"d0UwQ_A1LsOj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620597645118,"user_tz":420,"elapsed":926,"user":{"displayName":"Mar Gal","photoUrl":"","userId":"14196548149658065855"}},"outputId":"09f7e41c-5ec3-43f5-8f9c-43715e0d12ca"},"source":["CHECKPOINT_PATH = \"damaged_structures_detector/checkpoints/LinearModel_checkpoint.pth\"    # path of the saved checkpoint\n","loaded_checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)                      # print the parameters saved in the checkpoint"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'epoch': 6,\n"," 'model_state': OrderedDict([('linear1.weight',\n","               tensor([[ 0.0658,  0.0528,  0.0408,  ..., -0.0403, -0.0279, -0.0068],\n","                       [-0.0685, -0.0506, -0.0438,  ...,  0.0382,  0.0291,  0.0069]],\n","                      device='cuda:0')),\n","              ('linear1.bias', tensor([-2.4732,  2.4736], device='cuda:0'))]),\n"," 'optim_state': {'param_groups': [{'dampening': 0,\n","    'lr': 0.001,\n","    'momentum': 0.9,\n","    'nesterov': True,\n","    'params': [139815825681000, 139815825681072],\n","    'weight_decay': 0.001}],\n","  'state': {139815825681000: {'momentum_buffer': tensor([[-0.0674, -0.0668, -0.0662,  ..., -0.3024, -0.3098, -0.3107],\n","            [ 0.0674,  0.0669,  0.0662,  ...,  0.3024,  0.3098,  0.3107]],\n","           device='cuda:0')},\n","   139815825681072: {'momentum_buffer': tensor([ 0.2182, -0.2182], device='cuda:0')}}},\n"," 'val_acc': tensor(0.7051, device='cuda:0', dtype=torch.float64),\n"," 'val_loss': 18.30215422591746}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"sTbqWiw8zqIQ"},"source":["The next cell uses the loaded checkpoint to update our model, so that we do not have to train it from scratch."]},{"cell_type":"code","metadata":{"id":"hMG8jQW4MPqa"},"source":["# Update Parameters of the model\n","\n","model_ft.load_state_dict(loaded_checkpoint['model_state'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fwmuf_Vwz3WQ"},"source":["The next cell moves the model to the GPU memory were it will be trained. Then it prepares and prints the parameters that will be trained (based on our previous decision of feature extraction or finetuning). Afterwards in initiates or loads an optimizer for the training."]},{"cell_type":"code","metadata":{"id":"UoE-3WYOEdYy","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1594676013209,"user_tz":420,"elapsed":803,"user":{"displayName":"Mar Gal","photoUrl":"","userId":"14196548149658065855"}},"outputId":"437ebee1-fabb-4ea9-d76f-d2c1553581b8"},"source":["# Send the model to GPU\n","model_ft = model_ft.to(device)\n","\n","#  Gather the parameters to be optimized/updated in this run. If we are\n","#  finetuning we will be updating all parameters. However, if we are \n","#  doing feature extract method, we will only update the parameters\n","#  that we have just initialized, i.e. the parameters with requires_grad\n","#  is True.\n","params_to_update = model_ft.parameters()\n","print(\"Params to learn:\")\n","if FEATURE_EXTRACT:\n","    params_to_update = []\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","            print(\"\\t\",name)\n","else:\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            print(\"\\t\",name)\n","\n","# Initiate the optimizer (this one was used to create the model of the paper but below there are other options that were tried)\n","optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9,nesterov = True, weight_decay=1e-3)\n","\n","# Load optimizer parameters from checkpoint (uncomment if there is a checkpoint and you want to use its optimizer)\n","#optimizer_ft.load_state_dict(loaded_checkpoint['optim_state'])\n","\n","## If we want Adam optimizer\n","#optimizer_ft = optim.Adam(params_to_update, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n","\n","## Adagrad\n","#optimizer_ft = optim.Adagrad(params_to_update, lr=0.01, lr_decay=0, weight_decay=0, eps=1e-10)\n","\n","## RMSProp\n","#optimizer_ft = optim.RMSprop(params_to_update, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0)\n","\n","print(optimizer_ft)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Params to learn:\n","\t linear1.weight\n","\t linear1.bias\n","SGD (\n","Parameter Group 0\n","    dampening: 0\n","    lr: 0.001\n","    momentum: 0.9\n","    nesterov: True\n","    weight_decay: 0.001\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0eA2LDI-04_K"},"source":["The following cell defines the function that will be used for the training of the model"]},{"cell_type":"code","metadata":{"id":"EanJZkdc5N0T"},"source":["def train_model(model, dataloaders, criterion, optimizer, num_train, num_val, best_acc=100.0, num_epochs=25, save_checkpoint=False, is_inception=False):\n","    since = time.time()\n","\n","    val_acc_history = []\n","    val_loss_history = []\n","    train_acc_history = []\n","    train_loss_history = []\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    # Special case for inception because in training it has an auxiliary output. In train\n","                    #   mode we calculate the loss by summing the final output and the auxiliary output\n","                    #   but in testing we only consider the final output.\n","                    if is_inception and phase == 'train':\n","                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n","                        outputs, aux_outputs = model(inputs)\n","                        loss1 = criterion(outputs, labels)\n","                        loss2 = criterion(aux_outputs, labels)\n","                        loss = loss1 + 0.4*loss2\n","                    else:\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","                        \n","                    _, preds = torch.max(outputs, 1)\n","          \n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # train statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            \n","            \n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            if phase == 'train':\n","              epoch_loss = running_loss / num_train\n","              epoch_acc = running_corrects.double() / num_train\n","\n","            if phase == 'val':\n","              epoch_loss = running_loss / num_val\n","              epoch_acc = running_corrects.double() / num_val\n","\n","            \n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","               \n","                if save_checkpoint:\n","                  checkpoint = {\n","                     \"epoch\" : epoch,\n","                     \"model_state\" : model_ft.state_dict(),\n","                     \"optim_state\" : optimizer_ft.state_dict(),\n","                     \"val_loss\" : epoch_loss,\n","                     \"val_acc\" : epoch_acc\n","                  }\n","                  CHECKPOINT_PATH = \"damaged_structures_detector/checkpoints/checkpoint.pth\"  # if during an epoch of training a model performs better than the previous ones we create a checkpoint and save its parameters\n","                  torch.save(checkpoint, CHECKPOINT_PATH)\n","\n","                  MODEL_PATH = \"damaged_structures_detector/checkpoints/best_model.pth\"       # we also save the new best model in a form that it is ready to be loaded and used to predict\n","                  torch.save(model_ft, MODEL_PATH)\n","\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","                val_loss_history.append(epoch_loss)\n","            \n","            if phase == 'train':\n","                train_acc_history.append(epoch_acc)\n","                train_loss_history.append(epoch_loss)\n","\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # When training is over load the best model weights, and return it\n","    model.load_state_dict(best_model_wts)\n","    return model, (val_acc_history, val_loss_history, train_acc_history, train_loss_history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFVYWSdSUKHO"},"source":["# Number of epochs to train for \n","NUM_EPOCHS = 15\n","\n","# If the following parameter is True, then every time during training that the model has accuracy higher than the BEST_ACC, the model will be saved overwriting the previous best one.\n","SAVE_CHECKPOINT = True\n","\n","BEST_ACC = 0 # Best accuracy of current model. Replace with \"loaded_checkpoint['val_acc']\" if you are using a loaded model and not a new one!!!\n","\n","# Setup the loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# Train and evaluate\n","model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, NUM_TRAIN, NUM_VAL, BEST_ACC, NUM_EPOCHS, SAVE_CHECKPOINT, is_inception=(MODEL_NAME==\"inception\"))\n","\n","\n","# Plot the training curves of validation accuracy vs. number \n","#  of training epochs for the transfer learning method\n","\n","\n","plt.title(\" Accuracy vs. Number of Training Epochs\")\n","plt.xlabel(\"Training Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.plot(range(1,NUM_EPOCHS+1),hist[0],label=\"Validation\")\n","plt.plot(range(1,NUM_EPOCHS+1),hist[2],label=\"Train\")\n","plt.ylim((0,1.))\n","plt.xticks(np.arange(1, NUM_EPOCHS+1, 1.0))\n","plt.legend()\n","plt.show()\n","\n","\n","plt.title(\" Loss vs. Number of Training Epochs\")\n","plt.xlabel(\"Training Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.plot(range(1,NUM_EPOCHS+1),hist[1],label=\"Validation\")\n","plt.plot(range(1,NUM_EPOCHS+1),hist[3],label=\"Train\")\n","plt.xticks(np.arange(1, NUM_EPOCHS+1, 1.0))\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7HTlr9zi3X78"},"source":["All of the following cells demonstrate how to perform some useful actions related to saving/loading checkpoints and models. Note that none of the following cells are mandatory for training, but they can provide some useful tools for anyone that is interested in using this code!"]},{"cell_type":"markdown","metadata":{"id":"pO2imxWeFXJd"},"source":["Save Model or Checkpoint"]},{"cell_type":"code","metadata":{"id":"IW0hFLkHFZeI"},"source":["# FILE = \"model_ft.pth\"       # File name\n","# # torch.save(model_ft.state_dict(), FILE)   # Save the current state/parameter of the model\n","\n","# checkpoint = {           # note that you can add to the checkpoint whatever you think that might be useful\n","#     \"epoch\" : num_epochs,\n","#     \"model_state\" : model_ft.state_dict(),\n","#     \"optim_state\" : optimizer_ft.state_dict()\n","# }\n","# CHECKPOINT_PATH = \"damaged_structures_detector/checkpoints/checkpoint.pth\"\n","# torch.save(checkpoint, CHECKPOINT_PATH)\n","# MODEL_PATH = \"damaged_structures_detector/checkpoints/best_model.pth\"  # Here we are not saving the state of the model, but instead we are saving the model as an object. That means that when we load it we can use it easily but we can not change anything on it!\n","# torch.save(model_ft, MODEL_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EAuqpH1RGSW7"},"source":["Load Model (Needs initialization first, because here we are just loading the saved state of an old model to a new model)"]},{"cell_type":"code","metadata":{"id":"DdhsMflPGVwr"},"source":["# # loaded_model, input_size = initialize_model(model_name, num_classes, width, channels, feature_extract, use_pretrained=True)\n","# # loaded_model.load_state_dict(torch.load(FILE))\n","# # loaded_model.eval()\n","\n","# loaded_checkpoint = torch.load(CHECKPOINT_PATH)\n","# epoch = checkpoint[\"epoch\"]\n","# model_state = checkpoint[\"model_state\"]\n","# optim_state = checkpoint[\"optim_state\"]\n","\n","# optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n","\n","# optimizer_ft.load_state_dict(optim_state)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BtyUzybtJkgU"},"source":["Save on GPU, Load on CPU"]},{"cell_type":"code","metadata":{"id":"RVXjPkTLJeWS"},"source":["# device = torch.device(\"cuda\")\n","# model.to(device)\n","# torch.save(model.state_dict(), FILE)\n","\n","# device = torch.device('cpu')\n","# model = initialize_model(model_name, num_classes, width, channels, feature_extract, use_pretrained=True)\n","# model.load_state_dict(torch.load(FILE, map_location=device))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GN1IacSrKcbo"},"source":["Save GPU, Load GPU"]},{"cell_type":"code","metadata":{"id":"u0YSalMAKlys"},"source":["# device = torch.device(\"cuda\")\n","# model.to(device)\n","# torch.save(model.state_dict(), FILE)\n","\n","# model = initialize_model(model_name, num_classes, width, channels, feature_extract, use_pretrained=True)\n","# model.load_state_dict(torch.load(FILE))\n","# model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7UXqT_TbK922"},"source":["Save CPU, Load GPU"]},{"cell_type":"code","metadata":{"id":"j3plLslcLAgT"},"source":["# torch.save(model_ft.state_dict(), FILE)\n","\n","# device = torch.device(\"cuda\")\n","# model = initialize_model(model_name, num_classes, width, channels, feature_extract, use_pretrained=True)\n","# model.load_state_dict(torch.load(FILE, map_location=\"cuda:0\"))\n","# model.to(device)"],"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Use Model on New Dataset.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMpgdoS59R4TdmFQFZUJk59"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mGBILL7Gl-Oq"},"source":["The present script was developed and used on Google Colab. The purpose of the script is to allow the user to load the model described in our paper \"DamageMap: A post-wildfire damaged buildings classifier\", and use it to classify the images of a given dataset. The model will output \"0\" for an undamaged building, and \"1\" for a damaged building. \n","\n","The dataset should consist of separate images of building roofs, and all of the images should be contained in one folder. If the true labels of the dataset are known and the user wants to calculate the accuracy of the model, then the dataset should be prepared in the following way. \n","\n","Create a folder that contains 2 subfolders. The first subfolder (in alphabetical order) should contain the images of the undamaged buildings, because they will automatically get the label \"0\" (and we want it to match the prediction of our model for undamaged buildings). The second subfolder (in alphabetical order) should contain the images of damaged buildings."]},{"cell_type":"markdown","metadata":{"id":"2ZuPYjNRzUhV"},"source":["The following cell allows Google Colab to get access to the files of your Google Drive."]},{"cell_type":"code","metadata":{"id":"1bHg_jChyqqx","colab":{"base_uri":"https://localhost:8080/","height":138},"executionInfo":{"status":"ok","timestamp":1595528728395,"user_tz":420,"elapsed":34673,"user":{"displayName":"Mar Gal","photoUrl":"","userId":"14196548149658065855"}},"outputId":"c6995fa3-aa24-453b-bf53-93906c790ca4"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","%cd drive/My\\ Drive"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wGeSvbkfmc1f"},"source":["Importing *necessary* libraries."]},{"cell_type":"code","metadata":{"id":"RGYl1WyRzBjY","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1595528746644,"user_tz":420,"elapsed":3324,"user":{"displayName":"Mar Gal","photoUrl":"","userId":"14196548149658065855"}},"outputId":"295a3326-5406-4f96-ec60-44928d91cb53"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data import sampler, RandomSampler, SubsetRandomSampler\n","from torch.utils.tensorboard import SummaryWriter\n","from PIL import Image, ImageOps\n","import torchvision.datasets as dset\n","import torchvision.transforms as T\n","\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","import time\n","\n","import seaborn as sns\n","from __future__ import print_function \n","from __future__ import division\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["PyTorch Version:  1.5.1+cu101\n","Torchvision Version:  0.6.1+cu101\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"xt46IOWjme-W"},"source":["If a GPU is available then the following cell will allow our model to use it, to classify faster."]},{"cell_type":"code","metadata":{"id":"OmTfm7l-zMig","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595528752144,"user_tz":420,"elapsed":520,"user":{"displayName":"Mar Gal","photoUrl":"","userId":"14196548149658065855"}},"outputId":"11eb2ac3-fcf7-42c0-be38-64a1304cd34e"},"source":["USE_GPU = True\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","\n","print('using device:', device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using device: cpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"97Ao-WOcnqG7"},"source":["The following cell loads the dataset that the model will later classify."]},{"cell_type":"code","metadata":{"id":"lxDA-zZx1WFZ"},"source":["BATCH_SIZE = 64   # number of images that the model will be classifying in each step (limited by GPU or CPU capacity).\n","FOLDERNAME = 'damaged_structures_detector/xbd_for_prediction' # path to the folder with the images of the dataset we want to classify. \n","#If the folder contains subfolders, then the images in the first subfolder will automatically get the label \"0\", images in the next subfolder will get the label \"1\" and so on...\n","\n","## Following two lines contain the means and standard deviations (std) of the datasets described in our paper. Keep in mind that before classifying a new dataset\n","## it is necessary to normalize it using the mean and std of the dataset on which the model was trained on.\n","# Par and Carr: mean=[0.3662, 0.3452, 0.3384], std=[0.1552, 0.1500, 0.1475])\n","# Xbd: mean=[0.4597, 0.4655, 0.3800], std=[0.1425, 0.1265, 0.1287])\n","\n","data_transform = transforms.Compose([     # specifying the transformations that we will apply to the new dataset before classifying its' images\n","        transforms.Resize(224),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.4597, 0.4655, 0.3800],\n","                             std=[0.1425, 0.1265, 0.1287])   # since our model was trained on Xbd we normalize with the Xbd mean and std\n","    ])\n","\n","test_dataset = datasets.ImageFolder(FOLDERNAME, transform = data_transform)   # Apply transformations on the dataset\n","test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = 0) # Create the Pytorch dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EMkcpoWOrHa0"},"source":["The following cell repeats the transformed dataset creation and the creation of dataloader just because sometimes Google Colab would fail to load the whole dataset from the provided path. So, repeat one more time to be safe."]},{"cell_type":"code","metadata":{"id":"5ltbOjKG7Oxs"},"source":["test_dataset = datasets.ImageFolder(FOLDERNAME, transform = data_transform)\n","test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hd5GsH8urkmI"},"source":["Make sure that the loaded dataset contains all of the images in the specified folder."]},{"cell_type":"code","metadata":{"id":"F0yCukn_s9kQ","colab":{"base_uri":"https://localhost:8080/","height":185},"executionInfo":{"status":"ok","timestamp":1593700128394,"user_tz":420,"elapsed":45505,"user":{"displayName":"Mar Gal","photoUrl":"","userId":"14196548149658065855"}},"outputId":"224ce4c6-0cf7-46c5-b6fa-4d95fedc9411"},"source":["test_dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset ImageFolder\n","    Number of datapoints: 47543\n","    Root location: damaged_structures_detector/xbd_for_prediction\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=224, interpolation=PIL.Image.BILINEAR)\n","               CenterCrop(size=(224, 224))\n","               ToTensor()\n","               Normalize(mean=[0.3662, 0.3452, 0.3384], std=[0.1552, 0.15, 0.1475])\n","           )"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"ZUWjmdfCr9NU"},"source":["Load the model that will be used for prediction"]},{"cell_type":"code","metadata":{"id":"mZaNv-Jy11T_"},"source":["%%capture\n","MODEL_PATH = \"damaged_structures_detector/checkpoints/Resnet_model_trained_on_xbd.pth\"  # path to the model that will be used for classification\n","model = torch.load(MODEL_PATH, map_location=device)\n","model.to(device)\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ut1XPNEsRVB"},"source":["Classify the images of the dataset and calculate the accuracy of the classification (if true labels are available)."]},{"cell_type":"code","metadata":{"id":"md_jVyt-2OSH"},"source":["running_corrects = 0\n","\n","for inputs, labels in test_loader:\n","  inputs = inputs.to(device)\n","  labels = labels.to(device) # This line loads the true labels to later calculate the accuracy of the model\n","\n","  with torch.no_grad():\n","\n","     outputs  = model(inputs)\n","     _, preds = torch.max(outputs, 1) # Get model predictions\n","    \n","\n","  running_corrects += torch.sum(preds == labels.data) # Compare model predictions with true labels. This and the following step should be skipped if true labels are not known\n","\n","\n","\n","test_acc = running_corrects.double() / len(test_loader.dataset) # Calculate model prediction"],"execution_count":null,"outputs":[]}]}